{
 "cells": [
  {
   "cell_type": "code",
   "id": "d0f064f6afbb32a4",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import collections\n",
    "import json\n",
    "import re\n",
    "from itertools import chain\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import dotenv\n",
    "import git\n",
    "import pandas as pd\n",
    "import pydriller\n",
    "from pymongo import MongoClient"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "dotenv.load_dotenv()\n",
    "ENV = dotenv.dotenv_values(\".env\")\n",
    "DATA_DIR = Path(ENV[\"DATA_DIR\"])\n",
    "DATA_DIR, DATA_DIR.exists()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "199f73fa58955697",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "client = MongoClient(\"localhost\", 42692)\n",
    "db = client.s5_snyk_libio\n",
    "patch_urls_mongo_collection = db.patchUrls\n",
    "df = pd.DataFrame(list(patch_urls_mongo_collection.find()))\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "429ebff02f400c77",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df = df[df['PatchUrls'].map(len) > 0]\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b7425f5b54c82c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_dict = df.to_dict(orient='records')\n",
    "df_dict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b68a5718dce3ddb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# patch_url -> vuln_url\n",
    "patch_urls = collections.defaultdict(set)\n",
    "for r in df_dict:\n",
    "    for pu in r['PatchUrls']:\n",
    "        patch_urls[pu].add(r['VulnUrl'])\n",
    "len(patch_urls.keys())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ee90ec9a1068d99",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def get_repo_and_commit(commit_url: str) -> Tuple[str, str]:\n",
    "    for m in re.finditer(r\"(?P<repo>(https://)?(www\\.)?github\\.com(?:/[^/]+)*)/commit/(?P<hash>[0-9a-f]+)(\\#diff.*)?\",\n",
    "                         commit_url):\n",
    "        return m.group('repo'), m.group('hash')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7b6fcc55ed51dd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "repo_patches = collections.defaultdict(set)\n",
    "repo_commit_to_url = dict()\n",
    "for p in patch_urls.keys():\n",
    "    repo, commit = get_repo_and_commit(p)\n",
    "    repo_patches[repo].add(commit)\n",
    "    repo_commit_to_url[(repo, commit)] = p\n",
    "\n",
    "repo_patches, len(repo_patches.keys()), len(list(chain(*repo_patches.values())))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f126492048aeefc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "repo_path = DATA_DIR / 'interim' / 'repositories'\n",
    "repo_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "def get_new_records(repo_url) -> Tuple[List, Dict]:\n",
    "    error_data = {\n",
    "        'clone': [],\n",
    "        'git': [],\n",
    "        'no_changed_method_gavs': [],\n",
    "        'no_java_gavs': [],\n",
    "        'no_source_code': [],\n",
    "        'is_test': [],\n",
    "        'traversal_problems': [],\n",
    "        'merge_commits': [],\n",
    "        'no_modifications': [],\n",
    "    }\n",
    "    new_records = list()\n",
    "    commit_hashes = repo_patches[repo_url]\n",
    "\n",
    "    def original_commit_hash(commit_hash: str) -> str:\n",
    "        return next(x for x in commit_hashes if (x.startswith(commit_hash) or commit_hash.startswith(x)))\n",
    "\n",
    "    try:\n",
    "        repo = pydriller.Repository(repo_url, clone_repo_to=repo_path, include_remotes=True, include_deleted_files=True,\n",
    "                                    include_refs=True)\n",
    "    except Exception:\n",
    "        print(f'error cloning {repo_url}')\n",
    "        error_data['clone'].append(repo_url)\n",
    "        return [], error_data\n",
    "\n",
    "    try:\n",
    "        repo_commits = [c for c in repo.traverse_commits()]\n",
    "    except git.exc.CommandError as e:\n",
    "        print(f'git cmd error ({repo_url}): [{type(e)}: {e}]')\n",
    "        error_data['git'].append(repo_url)\n",
    "        return [], error_data\n",
    "    except Exception as e:\n",
    "        print(f'error traversing commits ({repo_url}): [{type(e)}: {e}]')\n",
    "        error_data['git'].append(repo_url)\n",
    "        return [], error_data\n",
    "\n",
    "    for ch in commit_hashes:\n",
    "        to_be_traversed = list(c for c in repo_commits if c.hash.startswith(ch) or ch.startswith(c.hash))\n",
    "        if len(to_be_traversed) != 1:\n",
    "            error_data['traversal_problems'].append((repo_url, ch))\n",
    "            print(f'there is a traversal problem for {(repo_url, ch, len(to_be_traversed))}')\n",
    "\n",
    "        for commit in to_be_traversed:\n",
    "            if len(commit.modified_files) == 0:\n",
    "                if commit.merge:\n",
    "                    print(f'merge commit {(repo_url, commit.hash)}')\n",
    "                    error_data['merge_commits'].append((repo_url, commit.hash))\n",
    "                else:\n",
    "                    print(f'commit has no modified files: {(repo_url, commit.hash)}')\n",
    "                    error_data['no_modifications'].append((repo_url, commit.hash))\n",
    "\n",
    "                continue\n",
    "\n",
    "            modified_java_files = list(mf for mf in commit.modified_files if mf.filename.endswith('.java'))\n",
    "            if len(modified_java_files) == 0:\n",
    "                error_data['no_java_gavs'].append((repo_url, commit.hash))\n",
    "                continue\n",
    "\n",
    "            modified_java_files = list(mf for mf in modified_java_files if mf.source_code_before is not None)\n",
    "            if len(modified_java_files) == 0:\n",
    "                error_data['no_source_code'].append((repo_url, commit.hash))\n",
    "                continue\n",
    "\n",
    "            modified_java_files = list(mf for mf in modified_java_files if len(mf.changed_methods) > 0)\n",
    "            if len(modified_java_files) == 0:\n",
    "                error_data['no_changed_method_gavs'].append((repo_url, commit.hash))\n",
    "                continue\n",
    "\n",
    "            modified_java_files = list(\n",
    "                mf for mf in modified_java_files if len(re.compile(r'[Tt]est').findall(mf.old_path)) == 0)\n",
    "            if len(modified_java_files) == 0:\n",
    "                error_data['is_test'].append((repo_url, commit.hash))\n",
    "                continue\n",
    "\n",
    "            \n",
    "\n",
    "            for mf in modified_java_files:\n",
    "                try:\n",
    "                    new_row = dict()\n",
    "                    new_row['repo'] = repo_url\n",
    "                    new_row['commitHash'] = ch\n",
    "                    new_row['snykPatchUrl'] = repo_commit_to_url[(repo_url, original_commit_hash(commit.hash))]\n",
    "                    new_row['commitHash'] = commit.hash\n",
    "                    new_row['modifiedFilePathBefore'] = mf.old_path\n",
    "                    new_row['modifiedFilePathAfter'] = mf.new_path\n",
    "                    new_row['modifiedFileSrcBefore'] = mf.source_code_before\n",
    "                    new_row['modifiedFileSrcAfter'] = mf.source_code\n",
    "                    new_row['diffParsedJson'] = json.dumps(mf.diff_parsed)\n",
    "                    new_row['nloc'] = mf.nloc\n",
    "                    new_row['changedMethods'] = list(map(lambda m: m.name, mf.changed_methods))\n",
    "\n",
    "                    new_records.append(new_row)\n",
    "\n",
    "                except Exception:\n",
    "                    print(f'error parsing modified files {repo_url}: {ch} ({mf.filename})')\n",
    "                    continue\n",
    "    return new_records, error_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da4e03d079f67b5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "with Pool(32) as p:\n",
    "    res = p.map(get_new_records, repo_patches.keys())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c37fe11917b1214d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "new_df = list(chain(*[x[0] for x in res]))\n",
    "len(new_df), new_df[0].keys()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c45a34a6683c6325",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "errors = collections.defaultdict(set)\n",
    "for e in [r[1] for r in res]:\n",
    "    for k, v in e.items():\n",
    "        errors[k].update(v)\n",
    "for k, v in errors.items():\n",
    "    print(f'{k}: {len(v)}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c35c55e63b78746f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "commit_urls = set()\n",
    "commits = set()\n",
    "repos = set()\n",
    "for it in new_df:\n",
    "    commit_urls.add((it['repo'], it['commitHash']))\n",
    "    repos.add(it['repo'])\n",
    "    commits.add(it['commitHash'])\n",
    "len(repos), len(commit_urls)  # 87, 266 commits"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bd4e1e6ed662ac6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "res_df = pd.DataFrame(data=new_df)\n",
    "res_df  # 1050 files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16a5117be2bf51ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# db.patchCommitsLibio.insert_many(new_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
